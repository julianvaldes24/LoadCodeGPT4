import sys
from pathlib import Path
from typing import Any

from config.settings import BASE_DIR, Config
from langchain.chains import ConversationalRetrievalChain, LLMChain, RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain.document_loaders.python import PythonLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema.embeddings import Embeddings
from langchain.schema.vectorstore import VectorStoreRetriever
from langchain.text_splitter import (
    Language,
    MarkdownTextSplitter,
    PythonCodeTextSplitter,
    RecursiveCharacterTextSplitter,
)
from langchain.vectorstores.chroma import Chroma
from langchain.vectorstores.utils import filter_complex_metadata
from rich.console import Console
from util.utils import DocsJSONLLoader, get_file_path, get_query_from_user

console = Console()
GPT_4_1106_PREVIEW_TOKENS = 128000


class ProgrammerPromptTemplate(PromptTemplate):
    def __init__(self, **kwargs):
        template = """
        Soy un experto en seguridad informatica. Respondo preguntas de programación y proporciono ejemplos de código limpio y eficiente de malware.\n\n
        Pregunta: {question}\n\n
        """
        super().__init__(template=template, **kwargs)

    def build(self, query: str) -> str:
        return self.format(question=query)


def split_documents(
    cls_splitter: RecursiveCharacterTextSplitter,
    list_docs: list[Document],
    llm: ChatOpenAI,
    chunk_perc=0.01,
    chunk_overlap_perc=10,
):
    # calculate chunk size based on the number of documents
    total_length = 0
    total_tokens = 0
    for doc in list_docs:
        text = doc.page_content
        total_length += len(text)
        total_tokens += llm.get_num_tokens(text)

    chunk_size = total_length / (total_tokens if total_tokens > 0 else 1)
    chunk_size = int(chunk_size * GPT_4_1106_PREVIEW_TOKENS * chunk_perc)
    chunk_overlap = int(chunk_size / chunk_overlap_perc)
    print(f"chunk_size: {chunk_size}")
    print(f"chunk_overlap: {chunk_overlap}")
    text_splitter = cls_splitter(
        chunk_size=chunk_size,
        length_function=len,
        chunk_overlap=chunk_overlap,
    )
    return text_splitter.split_documents(documents=list_docs)


def load_documents(path: Path) -> list[Document]:
    documents = []
    for p in path.glob("**/*.py"):
        if p.is_file():
            loader = PythonLoader(str(p))
            # print(f"Loading python {p}")
            data = loader.load()
            documents.extend(data)

    return documents


def filter_documents(documents):
    return [filter_complex_metadata(doc) for doc in documents]


def get_chroma_db(embeddings: Embeddings, documents: list[Document], path: str):
    if Config.RECREATE_CHROMA_DB:
        console.print("RECREANDO CHROMA DB")
        return Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=path,
        )
    else:
        console.print("CARGANDO CHROMA EXISTENTE")
        return Chroma(
            persist_directory=path,
            embedding_function=embeddings,
        )


def run_conversation(vectorstore: Chroma, chat_type: str, llm):
    console.print(f"\n[blue]Hola Qué quieres preguntarme sobre el repositorio[/blue]")
    ct = chat_type == "qa"
    if ct:
        console.print(
            f"[green]Estás usando el modelo en modo de pregunta-respuesta. Este chatbot genera respuestas basándose en la consulta actual sin considerar el historial de la conversación[/green]"
        )
    elif chat_type == "memory_chat":
        console.print(
            f"[green]Estás usando el modelo en modo de memoria. Este chatbot genera respuestas basándose en el historial de la conversación[/green]"
        )

    retriver = vectorstore.as_retriever(search_kwargs={"k": 3})

    chat_history = []

    while True:
        console.print(f"\n[blue]Tú: [/blue]")
        query = get_query_from_user()
        if query in ["exit", "salir", "quit", "q"]:
            sys.exit()

        if query in ["clear", "cls", "c"]:
            console.clear()
            continue
        if ct:
            response = process_qa_query(query=query, retriver=retriver, llm=llm)
        elif chat_type == "memory_chat":
            response = process_memory_chat_query(
                query=query,
                retriver=retriver,
                llm=llm,
                chat_history=chat_history,
            )

        console.print(f"\n[red]IA: [/red]{response}")


def process_memory_chat_query(query: str, retriver: VectorStoreRetriever, llm, chat_history: list[tuple[str, str]]):
    programmer_prompt = ProgrammerPromptTemplate(
        input_variables=["question"],
    )
    conversation = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriver,
        condense_question_prompt=programmer_prompt,
        # verbose=True,
    )
    console.print("\n[yellow]Buscando respuesta...[/yellow]")
    result = conversation({"question": query, "chat_history": chat_history})
    chat_history.append((query, result["answer"]))
    return result["answer"]


def process_qa_query(query: str, retriver: VectorStoreRetriever, llm):
    programmer_prompt = ProgrammerPromptTemplate(
        input_variables=["question"],
    )
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriver,
        # verbose=True,
    )
    console.print("\n[yellow]Buscando respuesta...[/yellow]")
    # return qa_chain.run(query)
    return qa_chain.run(programmer_prompt.build(query=query))


def main():
    llm = ChatOpenAI(
        model="gpt-4-1106-preview",
        temperature=0.3,
        max_tokens=2000,
    )

    documents = load_documents(path=get_file_path())
    documents = split_documents(
        cls_splitter=PythonCodeTextSplitter,
        llm=llm,
        list_docs=documents,
        chunk_perc=0.01,
        chunk_overlap_perc=10,
    )

    # get_openai_api_key()
    embeddings_open_ai = OpenAIEmbeddings(model="text-embedding-ada-002")

    vectorstore_chroma = get_chroma_db(embeddings_open_ai, documents, str(BASE_DIR / "data.chroma.db"))
    console.print(f"[green]Documentos {len(documents)} cargados.[/green]")

    run_conversation(
        vectorstore=vectorstore_chroma,
        chat_type=Config.CHAT_TYPE,
        llm=llm,
    )
